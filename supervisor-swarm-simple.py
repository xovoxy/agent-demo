"""
ç®€åŒ–çš„ LangGraph ç›‘ç£è€…æ¨¡å¼å’Œç¾¤ä½“æ™ºèƒ½å®ç°

åŸºäºç°æœ‰ä»£ç ç»“æ„ï¼Œä½¿ç”¨ LangGraph çš„ StateGraph å®ç°ï¼š
1. ç›‘ç£è€…æ¨¡å¼ï¼šæ™ºèƒ½ä»»åŠ¡è·¯ç”±
2. ç¾¤ä½“æ™ºèƒ½ï¼šå¹¶è¡Œåä½œå¤„ç†
"""

import asyncio
import os
import uuid
from typing import TypedDict, List, Dict, Any, Literal
from datetime import datetime

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage

load_dotenv()

# ==================== çŠ¶æ€å®šä¹‰ ====================

class TaskState(TypedDict, total=False):
    """ä»»åŠ¡çŠ¶æ€"""
    user_input: str
    task_type: str
    assigned_worker: str
    worker_result: str
    parallel_results: Dict[str, str]
    consensus_result: str
    messages: List[Any]
    step: int
    done: bool

# ==================== åˆå§‹åŒ– ====================

if not os.environ.get("DEEPSEEK_API_KEY"):
    os.environ["DEEPSEEK_API_KEY"] = input("Enter your DeepSeek API key: ")

model = init_chat_model("deepseek-chat", model_provider="deepseek")
search = TavilySearch(max_results=2)

# ==================== å·¥ä½œæ™ºèƒ½ä½“ ====================

def research_worker(input_text: str) -> str:
    """ç ”ç©¶å‹å·¥ä½œæ™ºèƒ½ä½“"""
    search_results = search.invoke(input_text)
    prompt = f"""
    åŸºäºæœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šï¼š
    
    ç”¨æˆ·é—®é¢˜: {input_text}
    æœç´¢ç»“æœ: {search_results}
    
    è¯·æä¾›ï¼š
    1. å…³é”®ä¿¡æ¯æ‘˜è¦
    2. è¯¦ç»†åˆ†æ
    3. ç›¸å…³å»ºè®®
    """
    response = model.invoke([{"role": "user", "content": prompt}])
    return response.content

def analysis_worker(input_text: str) -> str:
    """åˆ†æå‹å·¥ä½œæ™ºèƒ½ä½“"""
    prompt = f"""
    ä½œä¸ºä¸“ä¸šåˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ·±å…¥åˆ†æï¼š
    
    ç”¨æˆ·é—®é¢˜: {input_text}
    
    è¯·æä¾›ï¼š
    1. è¯¦ç»†åˆ†æ
    2. ä¼˜ç¼ºç‚¹æ¯”è¾ƒ
    3. å»ºè®®å’Œç»“è®º
    """
    response = model.invoke([{"role": "user", "content": prompt}])
    return response.content

def creative_worker(input_text: str) -> str:
    """åˆ›æ„å‹å·¥ä½œæ™ºèƒ½ä½“"""
    prompt = f"""
    ä½œä¸ºåˆ›æ„ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹è¯·æ±‚è¿›è¡Œåˆ›ä½œï¼š
    
    ç”¨æˆ·é—®é¢˜: {input_text}
    
    è¯·æä¾›ï¼š
    1. åˆ›æ„æ„æ€
    2. è¯¦ç»†å†…å®¹
    3. å®æ–½å»ºè®®
    """
    response = model.invoke([{"role": "user", "content": prompt}])
    return response.content

def technical_worker(input_text: str) -> str:
    """æŠ€æœ¯å‹å·¥ä½œæ™ºèƒ½ä½“"""
    prompt = f"""
    ä½œä¸ºæŠ€æœ¯ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹è¯·æ±‚æä¾›æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼š
    
    ç”¨æˆ·é—®é¢˜: {input_text}
    
    è¯·æä¾›ï¼š
    1. æŠ€æœ¯æ–¹æ¡ˆ
    2. å®ç°æ­¥éª¤
    3. æŠ€æœ¯å»ºè®®
    """
    response = model.invoke([{"role": "user", "content": prompt}])
    return response.content

# ==================== ç›‘ç£è€…æ¨¡å¼èŠ‚ç‚¹ ====================

def classify_task(state: TaskState) -> TaskState:
    """ä»»åŠ¡åˆ†ç±»èŠ‚ç‚¹"""
    user_input = state.get("user_input", "")
    
    classification_prompt = f"""
    åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œç¡®å®šæœ€é€‚åˆçš„å¤„ç†æ–¹å¼ï¼š
    ç”¨æˆ·è¯·æ±‚: {user_input}
    
    å¯é€‰ç±»å‹ï¼š
    - research: éœ€è¦æœç´¢ä¿¡æ¯ã€æŸ¥æ‰¾èµ„æ–™
    - analysis: éœ€è¦åˆ†æã€æ¯”è¾ƒã€è¯„ä¼°
    - creative: éœ€è¦åˆ›ä½œå†…å®¹ã€å†™ä½œ
    - technical: éœ€è¦æŠ€æœ¯å®ç°ã€ç¼–ç¨‹
    
    åªè¿”å›ä¸€ä¸ªå…³é”®è¯ï¼šresearch, analysis, creative, æˆ– technical
    """
    
    response = model.invoke([{"role": "user", "content": classification_prompt}])
    task_type = response.content.strip().lower()
    
    return {
        "task_type": task_type,
        "assigned_worker": task_type,
        "step": state.get("step", 0) + 1
    }

def assign_worker(state: TaskState) -> TaskState:
    """åˆ†é…å·¥ä½œèŠ‚ç‚¹"""
    task_type = state.get("assigned_worker", "analysis")
    user_input = state.get("user_input", "")
    
    workers = {
        "research": research_worker,
        "analysis": analysis_worker,
        "creative": creative_worker,
        "technical": technical_worker
    }
    
    if task_type in workers:
        worker_func = workers[task_type]
        result = worker_func(user_input)
        return {
            "worker_result": result,
            "done": True
        }
    else:
        return {
            "worker_result": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¿™ç§ç±»å‹çš„ä»»åŠ¡ã€‚",
            "done": True
        }

def route_after_classify(state: TaskState) -> Literal["assign", "finish"]:
    """åˆ†ç±»åçš„è·¯ç”±"""
    task_type = state.get("task_type", "")
    if task_type in ["research", "analysis", "creative", "technical"]:
        return "assign"
    return "finish"

# ==================== ç¾¤ä½“æ™ºèƒ½èŠ‚ç‚¹ ====================

def parallel_processing(state: TaskState) -> TaskState:
    """å¹¶è¡Œå¤„ç†èŠ‚ç‚¹"""
    user_input = state.get("user_input", "")
    
    # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰å·¥ä½œæ™ºèƒ½ä½“
    workers = {
        "research": research_worker,
        "analysis": analysis_worker,
        "creative": creative_worker,
        "technical": technical_worker
    }
    
    results = {}
    for worker_name, worker_func in workers.items():
        try:
            result = worker_func(user_input)
            results[worker_name] = result
        except Exception as e:
            results[worker_name] = f"å¤„ç†å¤±è´¥: {str(e)}"
    
    return {
        "parallel_results": results,
        "step": state.get("step", 0) + 1
    }

def build_consensus(state: TaskState) -> TaskState:
    """æ„å»ºå…±è¯†èŠ‚ç‚¹"""
    parallel_results = state.get("parallel_results", {})
    user_input = state.get("user_input", "")
    
    consensus_prompt = f"""
    ç»¼åˆå¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“çš„è§‚ç‚¹ï¼š
    
    ç”¨æˆ·é—®é¢˜: {user_input}
    
    ç ”ç©¶ä¸“å®¶: {parallel_results.get('research', '')}
    åˆ†æå¸ˆ: {parallel_results.get('analysis', '')}
    åˆ›æ„ä¸“å®¶: {parallel_results.get('creative', '')}
    æŠ€æœ¯ä¸“å®¶: {parallel_results.get('technical', '')}
    
    è¯·ç»¼åˆè¿™äº›è§‚ç‚¹ï¼Œæä¾›ä¸€ä¸ªå¹³è¡¡ã€å…¨é¢çš„æœ€ç»ˆç­”æ¡ˆã€‚
    """
    
    response = model.invoke([{"role": "user", "content": consensus_prompt}])
    
    return {
        "consensus_result": response.content,
        "done": True
    }

# ==================== å›¾æ„å»º ====================

def build_supervisor_graph():
    """æ„å»ºç›‘ç£è€…æ¨¡å¼å›¾"""
    graph = StateGraph(TaskState)
    
    graph.add_node("classify", classify_task)
    graph.add_node("assign", assign_worker)
    
    graph.add_edge(START, "classify")
    graph.add_conditional_edges("classify", route_after_classify, {
        "assign": "assign",
        "finish": END
    })
    graph.add_edge("assign", END)
    
    return graph.compile(checkpointer=MemorySaver())

def build_swarm_graph():
    """æ„å»ºç¾¤ä½“æ™ºèƒ½å›¾"""
    graph = StateGraph(TaskState)
    
    graph.add_node("parallel", parallel_processing)
    graph.add_node("consensus", build_consensus)
    
    graph.add_edge(START, "parallel")
    graph.add_edge("parallel", "consensus")
    graph.add_edge("consensus", END)
    
    return graph.compile(checkpointer=MemorySaver())

# ==================== æ¼”ç¤ºå‡½æ•° ====================

async def demo_supervisor():
    """æ¼”ç¤ºç›‘ç£è€…æ¨¡å¼"""
    print("ğŸ¤– ç›‘ç£è€…æ¨¡å¼æ¼”ç¤º")
    print("=" * 50)
    
    supervisor_graph = build_supervisor_graph()
    config = {"configurable": {"thread_id": f"supervisor-{uuid.uuid4()}"}}
    
    test_cases = [
        "è¯·ç ”ç©¶ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•è¶‹åŠ¿",
        "åˆ†æä¸€ä¸‹Pythonå’ŒJavaçš„ä¼˜ç¼ºç‚¹", 
        "å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
        "è®¾è®¡ä¸€ä¸ªç®€å•çš„ç”¨æˆ·ç™»å½•ç³»ç»Ÿ"
    ]
    
    for i, query in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {query}")
        print("-" * 30)
        
        result = supervisor_graph.invoke({
            "user_input": query,
            "step": 0,
            "done": False
        }, config)
        
        print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {result.get('task_type')}")
        print(f"ğŸ‘¤ åˆ†é…æ™ºèƒ½ä½“: {result.get('assigned_worker')}")
        print(f"ğŸ“‹ ç»“æœ: {result.get('worker_result', '')[:200]}...")

async def demo_swarm():
    """æ¼”ç¤ºç¾¤ä½“æ™ºèƒ½"""
    print("\nğŸ ç¾¤ä½“æ™ºèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    swarm_graph = build_swarm_graph()
    config = {"configurable": {"thread_id": f"swarm-{uuid.uuid4()}"}}
    
    test_cases = [
        "å¦‚ä½•æé«˜å›¢é˜Ÿçš„å·¥ä½œæ•ˆç‡ï¼Ÿ",
        "æœªæ¥5å¹´æœ€æœ‰å‰æ™¯çš„æŠ€æœ¯é¢†åŸŸæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for i, query in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {query}")
        print("-" * 30)
        
        result = swarm_graph.invoke({
            "user_input": query,
            "step": 0,
            "done": False
        }, config)
        
        print("ğŸ” å„æ™ºèƒ½ä½“è§‚ç‚¹:")
        parallel_results = result.get("parallel_results", {})
        for agent_name, agent_result in parallel_results.items():
            print(f"  {agent_name}: {agent_result[:100]}...")
        
        print(f"\nğŸ¤ ç¾¤ä½“å…±è¯†: {result.get('consensus_result', '')[:200]}...")

async def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("\nğŸ® äº¤äº’å¼æ¼”ç¤º")
    print("=" * 50)
    
    while True:
        print("\né€‰æ‹©æ¨¡å¼:")
        print("1. ç›‘ç£è€…æ¨¡å¼ (æ™ºèƒ½è·¯ç”±)")
        print("2. ç¾¤ä½“æ™ºèƒ½ (å¹¶è¡Œåä½œ)")
        print("3. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "1":
            query = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
            supervisor_graph = build_supervisor_graph()
            config = {"configurable": {"thread_id": f"interactive-{uuid.uuid4()}"}}
            
            result = supervisor_graph.invoke({
                "user_input": query,
                "step": 0,
                "done": False
            }, config)
            
            print(f"\nğŸ¯ ä»»åŠ¡ç±»å‹: {result.get('task_type')}")
            print(f"ğŸ‘¤ åˆ†é…æ™ºèƒ½ä½“: {result.get('assigned_worker')}")
            print(f"ğŸ“‹ å¤„ç†ç»“æœ:\n{result.get('worker_result')}")
            
        elif choice == "2":
            query = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
            swarm_graph = build_swarm_graph()
            config = {"configurable": {"thread_id": f"interactive-{uuid.uuid4()}"}}
            
            result = swarm_graph.invoke({
                "user_input": query,
                "step": 0,
                "done": False
            }, config)
            
            print(f"\nğŸ¤ ç¾¤ä½“å…±è¯†:\n{result.get('consensus_result')}")
            
        elif choice == "3":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LangGraph ç›‘ç£è€…æ¨¡å¼å’Œç¾¤ä½“æ™ºèƒ½æ¼”ç¤º")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # æ¼”ç¤ºç›‘ç£è€…æ¨¡å¼
        await demo_supervisor()
        
        # æ¼”ç¤ºç¾¤ä½“æ™ºèƒ½
        await demo_swarm()
        
        # äº¤äº’å¼æ¼”ç¤º
        await interactive_demo()
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    asyncio.run(main())
